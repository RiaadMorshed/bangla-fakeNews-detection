{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68245265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.8.0-cp38-cp38-win_amd64.whl (438.0 MB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\riaad\\anaconda3\\lib\\site-packages (from tensorflow) (52.0.0.post20210125)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\riaad\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\riaad\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Collecting absl-py>=0.4.0\n",
      "  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.24.0-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\riaad\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\riaad\\anaconda3\\lib\\site-packages (from tensorflow) (1.20.1)\n",
      "Collecting tensorboard<2.9,>=2.8\n",
      "  Using cached tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\riaad\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Collecting flatbuffers>=1.12\n",
      "  Using cached flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting gast>=0.2.1\n",
      "  Using cached gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.20.0-cp38-cp38-win_amd64.whl (904 kB)\n",
      "Collecting keras<2.9,>=2.8.0rc0\n",
      "  Using cached keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Collecting libclang>=9.0.1\n",
      "  Using cached libclang-13.0.0-py2.py3-none-win_amd64.whl (13.9 MB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.44.0-cp38-cp38-win_amd64.whl (3.4 MB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Using cached tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\riaad\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.36.2)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\riaad\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.25.1)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.6.2-py2.py3-none-any.whl (156 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\riaad\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Using cached importlib_metadata-4.11.3-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\riaad\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.4.1)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\riaad\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\riaad\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\riaad\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\riaad\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=a4f371d044d652b2c3037a8e2601a90505416a21c09c82061d32202046700a3e\n",
      "  Stored in directory: c:\\users\\riaad\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built termcolor\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, tf-estimator-nightly, termcolor, tensorflow-io-gcs-filesystem, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 3.10.0\n",
      "    Uninstalling importlib-metadata-3.10.0:\n",
      "      Successfully uninstalled importlib-metadata-3.10.0\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-5.0.0 flatbuffers-2.0 gast-0.5.3 google-auth-2.6.2 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.44.0 importlib-metadata-4.11.3 keras-2.8.0 keras-preprocessing-1.1.2 libclang-13.0.0 markdown-3.3.6 oauthlib-3.2.0 opt-einsum-3.3.0 protobuf-3.20.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.8 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tensorflow-io-gcs-filesystem-0.24.0 termcolor-1.1.0 tf-estimator-nightly-2.8.0.dev2021122109\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86acffa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import one_hot,Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense , Flatten ,Embedding,Input,LSTM, Bidirectional,GRU\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88726825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>চট্টগ্রাম প্রিমিয়ার বিশ্ববিদ্যালয়ের দামপাড়া ভব...</td>\n",
       "      <td>1</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>কক্সবাজারের রামুতে বৌদ্ধ বিহার বসতিতে হামলার স...</td>\n",
       "      <td>1</td>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>গার্লফ্রেন্ড দেয়া জিপিএ পেলে যেনো মায়ের হাতের ...</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>কারণে বিখ্যাত বন্দর নগরী চট্টগ্রাম পাশ্চাত্যের...</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>বাম গণতান্ত্রিক জোটের নির্বাচন কমিশন ইসি কার্য...</td>\n",
       "      <td>1</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            content  label  \\\n",
       "0           0  চট্টগ্রাম প্রিমিয়ার বিশ্ববিদ্যালয়ের দামপাড়া ভব...      1   \n",
       "1           1  কক্সবাজারের রামুতে বৌদ্ধ বিহার বসতিতে হামলার স...      1   \n",
       "2           2  গার্লফ্রেন্ড দেয়া জিপিএ পেলে যেনো মায়ের হাতের ...      0   \n",
       "3           3  কারণে বিখ্যাত বন্দর নগরী চট্টগ্রাম পাশ্চাত্যের...      0   \n",
       "4           4  বাম গণতান্ত্রিক জোটের নির্বাচন কমিশন ইসি কার্য...      1   \n",
       "\n",
       "   length  \n",
       "0     157  \n",
       "1     386  \n",
       "2      76  \n",
       "3     112  \n",
       "4     146  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel('./dataset/preprocessed.xlsx')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8597081",
   "metadata": {},
   "source": [
    "# Word Embeddings(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2baf2121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words =  391114\n",
      "Total Sentances =  2000\n"
     ]
    }
   ],
   "source": [
    "corpus=[]\n",
    "words = 0;\n",
    "j = 0\n",
    "for i in data['content'].values:\n",
    "    corpus.append(str(i).split(\" \"))\n",
    "    words += len(corpus[j])\n",
    "    j += 1\n",
    "print(\"Total words = \", words)\n",
    "print(\"Total Sentances = \", len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce5a612c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ model loading ....\n",
      "ℹ vocab building with new sentences\n",
      "ℹ pre-training started.......\n",
      "ℹ please wait.....it will take time according to your data size and computation\n",
      "capability\n",
      "✔ pre-train completed successfully\n",
      "✔ pre-trianing loss: 0.0\n",
      "ℹ model and vector saving...\n",
      "✔ model and vector saved as ./word_embeddings/new_w2v.model and\n",
      "./word_embeddings/new_w2v_vector.vector\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bnlp import BengaliWord2Vec\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "bwv = BengaliWord2Vec()\n",
    "\n",
    "trained_model_path = \"./word_embeddings/bnwiki_word2vec.model\"\n",
    "data_file = corpus\n",
    "model_name = \"./word_embeddings/new_w2v.model\"\n",
    "vector_name = \"./word_embeddings/new_w2v_vector.vector\"\n",
    "bwv.pretrain(trained_model_path, data_file, model_name, vector_name, epochs=10)\n",
    "\n",
    "\n",
    "w2v_model = Word2Vec.load(\"./word_embeddings/new_w2v.model\")\n",
    "w2v_model.train(corpus,total_words=391114, epochs=10)\n",
    "\n",
    "w2v_model.wv.most_similar('নষ্ট')\n",
    "w2v_model.wv['বাংলাদেশ'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9b429c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './word_embeddings/fakeNews_word2vec_embeddings.txt'\n",
    "w2v_model.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd2f51d",
   "metadata": {},
   "source": [
    "# Data processing for models and embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15e42e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary :  52234\n",
      "All sentances with same length  (2000, 300)\n",
      "[0 1]\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "import os\n",
    "\n",
    "# tokenize every words so that evey words maps to numaric value\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(data['content'].values.astype('U'))\n",
    "encd_rev = tok.texts_to_sequences(data['content'].values.astype('U'))\n",
    "# tok.word_index\n",
    "\n",
    "# make all the input sentance same length with add padding\n",
    "max_rev_len = 40 # max lenght of a sentance\n",
    "vocab_size = len(tok.word_index) + 1  # total no of words\n",
    "embed_dim = 100 # embedding dimension \n",
    "pad_rev= pad_sequences(encd_rev, maxlen=, padding='post')\n",
    "\n",
    "print(\"Size of vocabulary : \", vocab_size)\n",
    "print(\"All sentances with same length \", pad_rev.shape)\n",
    "\n",
    "# lebel encode the output label to categorical\n",
    "Y = data['label']\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "print(encoder.classes_)\n",
    "print(dummy_y)\n",
    "\n",
    "# test train split\n",
    "X_train_word,X_test_word,y_train_word,y_test_word=train_test_split(pad_rev,dummy_y,test_size=0.30, random_state = 0)\n",
    "\n",
    "\n",
    "# make a dictionary. word as key and feature vector as value\n",
    "embedding_index={}\n",
    "f = open('./word_embeddings/fakeNews_word2vec_embeddings.txt',encoding='utf-8')\n",
    "for line in f:\n",
    "    values=line.split()\n",
    "    word=values[0]\n",
    "    coefs=np.asarray(values[1:])\n",
    "    embedding_index[word]=coefs\n",
    "f.close()\n",
    "\n",
    "# create a embeddings matrix with 100 dimenstion\n",
    "EMBEDDING_DIM=100\n",
    "embedding_matrix=np.zeros((vocab_size,EMBEDDING_DIM))\n",
    "for word, i in tok.word_index.items():\n",
    "    if i>vocab_size:\n",
    "        continue\n",
    "    embedding_vector=embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i]=embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6d4f15",
   "metadata": {},
   "source": [
    "# Model Training and Result evaluation helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "381da07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "def print_result(model, history, x_test, y_test, name):\n",
    "    model_path = \"./Models/\"+ name+ \" model\"\n",
    "    \n",
    "    # convert the history.history dict to a pandas DataFrame:     \n",
    "    hist_df = pd.DataFrame(history.history) \n",
    "\n",
    "    # save to json:  \n",
    "    hist_json_file = \"./History/\" + name + 'history.json' \n",
    "    with open(hist_json_file, mode='w') as f:\n",
    "        hist_df.to_json(f)\n",
    "            \n",
    "    #save model\n",
    "    model.save(model_path)\n",
    "    \n",
    "    \n",
    "    #ploting training  history\n",
    "    accr = model.evaluate(x_test,y_test)\n",
    "    print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    #normalize\n",
    "    y_pred_frac=np.argmax(y_pred, axis=1)\n",
    "    y_test_frac=np.argmax(y_test, axis=1)\n",
    "\n",
    "    print(\"-------------Classification Report----------------\")\n",
    "    print(classification_report(y_test_frac, y_pred_frac))\n",
    "    print(\"-------------------//*//-------------------------\")\n",
    "\n",
    "    print(\"-------------Confusion Matrix----------------\")\n",
    "    cm = confusion_matrix(y_test_frac, y_pred_frac)\n",
    "    print(cm)\n",
    "    print(\"-------------------//*//-------------------------\")\n",
    "    \n",
    "    \n",
    "    \n",
    "def train_model(model, X_train, y_train):\n",
    "    epochs = 10\n",
    "    batch_size = 64\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2f7c6e",
   "metadata": {},
   "source": [
    "# Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5c1271a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary of the built model..\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 300, 100)          5223400   \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirectio  (None, 256)              234496    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,458,410\n",
      "Trainable params: 235,010\n",
      "Non-trainable params: 5,223,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "embedding_layer=Embedding(vocab_size,\n",
    "                        EMBEDDING_DIM,\n",
    "                        embeddings_initializer=Constant(embedding_matrix),\n",
    "                        input_length=300,\n",
    "                        trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(Bidirectional(LSTM(units=128,dropout=0.2,recurrent_dropout=0.2)))\n",
    "model.add(Dense(2,activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print('summary of the built model..')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43ca65a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20/20 [==============================] - 127s 6s/step - loss: 0.5390 - accuracy: 0.7325 - val_loss: 0.5069 - val_accuracy: 0.7714\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 124s 6s/step - loss: 0.3478 - accuracy: 0.8571 - val_loss: 0.3765 - val_accuracy: 0.8500\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 127s 6s/step - loss: 0.2611 - accuracy: 0.8944 - val_loss: 0.4659 - val_accuracy: 0.8214\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 128s 6s/step - loss: 0.2303 - accuracy: 0.9008 - val_loss: 0.4855 - val_accuracy: 0.8071\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 129s 6s/step - loss: 0.1876 - accuracy: 0.9246 - val_loss: 0.5113 - val_accuracy: 0.8286\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 129s 6s/step - loss: 0.1449 - accuracy: 0.9516 - val_loss: 0.5054 - val_accuracy: 0.8214\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 130s 7s/step - loss: 0.1085 - accuracy: 0.9643 - val_loss: 0.5051 - val_accuracy: 0.8143\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 132s 7s/step - loss: 0.0922 - accuracy: 0.9722 - val_loss: 0.5177 - val_accuracy: 0.8214\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 151s 8s/step - loss: 0.0849 - accuracy: 0.9722 - val_loss: 0.4908 - val_accuracy: 0.8643\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 153s 8s/step - loss: 0.0856 - accuracy: 0.9690 - val_loss: 0.4477 - val_accuracy: 0.8286\n"
     ]
    }
   ],
   "source": [
    "history, model = train_model(model, X_train_word, y_train_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a5adfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./Models/LSTM model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000138471375B0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000013847137A00> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 5s 220ms/step - loss: 0.6689 - accuracy: 0.6050\n",
      "Test set\n",
      "  Loss: 0.669\n",
      "  Accuracy: 0.605\n",
      "-------------Classification Report----------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.67      0.63       306\n",
      "           1       0.61      0.54      0.57       294\n",
      "\n",
      "    accuracy                           0.60       600\n",
      "   macro avg       0.61      0.60      0.60       600\n",
      "weighted avg       0.61      0.60      0.60       600\n",
      "\n",
      "-------------------//*//-------------------------\n",
      "-------------Confusion Matrix----------------\n",
      "[[204 102]\n",
      " [135 159]]\n",
      "-------------------//*//-------------------------\n"
     ]
    }
   ],
   "source": [
    "print_result(model, history, X_test_word, y_test_word, 'LSTM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8effc5b5",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b6f0466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary of the built model..\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     (None, 300, 100)          5223400   \n",
      "                                                                 \n",
      " bidirectional_6 (Bidirectio  (None, 256)              176640    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,400,554\n",
      "Trainable params: 177,154\n",
      "Non-trainable params: 5,223,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "embedding_layer=Embedding(vocab_size,\n",
    "                        EMBEDDING_DIM,\n",
    "                        embeddings_initializer=Constant(embedding_matrix),\n",
    "                        input_length=300,\n",
    "                        trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(Bidirectional(GRU(units=128,dropout=0.2,recurrent_dropout=0.2)))\n",
    "model.add(Dense(2,activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print('summary of the built model..')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c8b6575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20/20 [==============================] - 106s 5s/step - loss: 0.6203 - accuracy: 0.6579 - val_loss: 0.6021 - val_accuracy: 0.7214\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 97s 5s/step - loss: 0.4682 - accuracy: 0.7825 - val_loss: 0.5410 - val_accuracy: 0.7429\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 91s 5s/step - loss: 0.3839 - accuracy: 0.8294 - val_loss: 0.5110 - val_accuracy: 0.7929\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 90s 4s/step - loss: 0.3219 - accuracy: 0.8587 - val_loss: 0.4965 - val_accuracy: 0.8000\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 91s 5s/step - loss: 0.2653 - accuracy: 0.8937 - val_loss: 0.5156 - val_accuracy: 0.7643\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 92s 5s/step - loss: 0.2170 - accuracy: 0.9190 - val_loss: 0.4924 - val_accuracy: 0.8071\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 90s 5s/step - loss: 0.1763 - accuracy: 0.9429 - val_loss: 0.5146 - val_accuracy: 0.7929\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 91s 5s/step - loss: 0.1578 - accuracy: 0.9492 - val_loss: 0.4788 - val_accuracy: 0.8143\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 91s 5s/step - loss: 0.1260 - accuracy: 0.9587 - val_loss: 0.5313 - val_accuracy: 0.8143\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 90s 5s/step - loss: 0.1101 - accuracy: 0.9619 - val_loss: 0.5857 - val_accuracy: 0.7714\n"
     ]
    }
   ],
   "source": [
    "history, model = train_model(model, X_train_word, y_train_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3dad433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./Models/GRU model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./Models/GRU model\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x00000138726D19A0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x00000138726D1AC0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 6s 340ms/step - loss: 0.4108 - accuracy: 0.8667\n",
      "Test set\n",
      "  Loss: 0.411\n",
      "  Accuracy: 0.867\n",
      "-------------Classification Report----------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.83      0.86       306\n",
      "           1       0.84      0.90      0.87       294\n",
      "\n",
      "    accuracy                           0.87       600\n",
      "   macro avg       0.87      0.87      0.87       600\n",
      "weighted avg       0.87      0.87      0.87       600\n",
      "\n",
      "-------------------//*//-------------------------\n",
      "-------------Confusion Matrix----------------\n",
      "[[255  51]\n",
      " [ 29 265]]\n",
      "-------------------//*//-------------------------\n"
     ]
    }
   ],
   "source": [
    "print_result(model, history, X_test_word, y_test_word, 'GRU')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
